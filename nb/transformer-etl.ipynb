{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from molfeat.trans.pretrained.hf_transformers import HFExperiment\n",
    "from molfeat.trans.pretrained.hf_transformers import HFModel\n",
    "from molfeat.store import ModelInfo\n",
    "from molfeat.store import ModelStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelInfo(name='gin_supervised_contextpred', inputs='smiles', type='pretrained', version=0, group='dgllife', submitter='Datamol', description='GIN neural network model pre-trained with supervised learning and context prediction on molecules from ChEMBL.', representation='graph', require_3D=False, tags=['GIN', 'dgl', 'pytorch', 'graph'], authors=['Weihua Hu', 'Bowen Liu', 'Joseph Gomes', 'Marinka Zitnik', 'Percy Liang', 'Vijay Pande', 'Jure Leskovec'], reference='https://arxiv.org/abs/1905.12265', created_at=datetime.datetime(2023, 2, 2, 19, 51, 17, 228390), sha256sum='72dc062936b78b515ed5d0989f909ab7612496d698415d73826b974c9171504a'),\n",
       " ModelInfo(name='jtvae_zinc_no_kl', inputs='smiles', type='pretrained', version=0, group='dgllife', submitter='Datamol', description='A JTVAE pre-trained on ZINC for molecule generation, without KL regularization', representation='other', require_3D=False, tags=['JTNN', 'JTVAE', 'dgl', 'pytorch', 'junction-tree', 'graph'], authors=['Wengong Jin', 'Regina Barzilay', 'Tommi Jaakkola'], reference='https://arxiv.org/abs/1802.04364v4', created_at=datetime.datetime(2023, 2, 2, 19, 51, 20, 468939), sha256sum='eab8ecb8a7542a8cdf97410cb27f72aaf374fefef6a1f53642cc5b310cf2b7f6'),\n",
       " ModelInfo(name='pcqm4mv2_graphormer_base', inputs='smiles', type='pretrained', version=0, group='graphormer', submitter='Datamol', description='Pretrained Graph Transformer on PCQM4Mv2 Homo-Lumo energy gap prediction using 2D molecular graphs.', representation='graph', require_3D=False, tags=['Graphormer', 'PCQM4Mv2', 'graph', 'pytorch', 'Microsoft'], authors=['Chengxuan Ying', 'Tianle Cai', 'Shengjie Luo', 'Shuxin Zheng', 'Guolin Ke', 'Di He', 'Yanming Shen', 'Tie-Yan Liu'], reference='https://arxiv.org/abs/2106.05234', created_at=datetime.datetime(2023, 2, 2, 19, 51, 19, 330147), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1'),\n",
       " ModelInfo(name='DeepChem-ChemBERTa-77M-MLM', inputs='smiles', type='pretrained', version=0, group='huggingface', submitter='Datamol', description='ChemBERTa is a pre-trained language model for molecules based on (Ro)BERT(a) trained on PubChem compounds. The MTR version was pretrained using mutitask regression objective, while the MLM version was pretrained using a masked language modeling objective', representation='line-notation', require_3D=False, tags=['ChemBERTa-2', 'smiles', 'huggingface', 'transformers', 'MLM', 'RoBERTa', 'PubChem'], authors=['Walid Ahmad', 'Elana Simon', 'Seyone Chithrananda', 'Gabriel Grand', 'Bharath Ramsundar'], reference='https://arxiv.org/abs/2209.01712', created_at=datetime.datetime(2023, 2, 3, 0, 39, 16, 581310), sha256sum='e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'),\n",
       " ModelInfo(name='DeepChem-ChemBERTa-77M-MTR', inputs='smiles', type='pretrained', version=0, group='huggingface', submitter='Datamol', description='ChemBERTa is a pre-trained language model for molecules based on (Ro)BERT(a) trained on PubChem compounds. The MTR version was pretrained using mutitask regression objective, while the MLM version was pretrained using a masked language modeling objective', representation='line-notation', require_3D=False, tags=['ChemBERTa-2', 'smiles', 'huggingface', 'transformers', 'MTR', 'RoBERTa', 'PubChem'], authors=['Walid Ahmad', 'Elana Simon', 'Seyone Chithrananda', 'Gabriel Grand', 'Bharath Ramsundar'], reference='https://arxiv.org/abs/2209.01712', created_at=datetime.datetime(2023, 2, 3, 0, 39, 46, 995385), sha256sum='e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'),\n",
       " ModelInfo(name='maccs', inputs='smiles', type='hand-crafted', version=0, group='rdkit', submitter='Datamol', description='MACCS keys are 166-bit 2D structure fingerprints that are commonly used for the measure of molecular similarity. They described the presence of key features in molecular graphs', representation='vector', require_3D=False, tags=['maccs', 'fixed', '2D', 'binary', 'rdkit'], authors=['MDL Information Systems'], reference='https://doi.org/10.1021/ci010132r', created_at=datetime.datetime(2023, 2, 2, 19, 51, 10, 688803), sha256sum='9c298d589a2158eb513cb52191144518a2acab2cb0c04f1df14fca0f712fa4a1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = ModelStore()\n",
    "store.available_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Pretrained Model saving"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChemBerta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemberta_77_mlm_card = ModelInfo(\n",
    "    name = \"DeepChem-ChemBERTa-77M-MLM\",\n",
    "    inputs = \"smiles\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"ChemBERTa is a pre-trained language model for molecules based on (Ro)BERT(a) trained on PubChem compounds. The MTR version was pretrained using mutitask regression objective, while the MLM version was pretrained using a masked language modeling objective\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"ChemBERTa-2\", \"smiles\",  'huggingface', \"transformers\", \"MLM\", \"RoBERTa\", \"PubChem\"],\n",
    "    authors= [\"Walid Ahmad\", \"Elana Simon\", \"Seyone Chithrananda\", \"Gabriel Grand\", \"Bharath Ramsundar\"],\n",
    "    reference = \"https://arxiv.org/abs/2209.01712\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71013f8791547b5b3ed7ca601527c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 00:39:33.338 | INFO     | molfeat.trans.pretrained.hf_transformers:save:49 - Model saved to gs://molfeat-store-dev/artifacts/huggingface/DeepChem-ChemBERTa-77M-MLM/0/model.save\n",
      "2023-02-03 00:39:35.003 | INFO     | molfeat.store.modelstore:register:124 - Successfuly registered model DeepChem-ChemBERTa-77M-MLM !\n"
     ]
    }
   ],
   "source": [
    "# attempt to register the model\n",
    "model = HFModel.register_pretrained(\"DeepChem/ChemBERTa-77M-MLM\", \"DeepChem/ChemBERTa-77M-MLM\", chemberta_77_mlm_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemberta_77_mtr_card = ModelInfo(\n",
    "    name = \"DeepChem-ChemBERTa-77M-MTR\",\n",
    "    inputs = \"smiles\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"ChemBERTa is a pre-trained language model for molecules based on (Ro)BERT(a) trained on PubChem compounds. The MTR version was pretrained using mutitask regression objective, while the MLM version was pretrained using a masked language modeling objective\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"ChemBERTa-2\", \"smiles\",  'huggingface', \"transformers\", \"MTR\", \"RoBERTa\", \"PubChem\"],\n",
    "    authors= [\"Walid Ahmad\", \"Elana Simon\", \"Seyone Chithrananda\", \"Gabriel Grand\", \"Bharath Ramsundar\"],\n",
    "    reference = \"https://arxiv.org/abs/2209.01712\" \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/config.json from cache at /Users/manu/.cache/huggingface/transformers/b4baac35f554b160534b510a71ab3ab508601f5277963a9e6e634b957411009d.75548765cc29be14dbfd305bced35cb576919329f1680b8be2b4b7c2749c85bd\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MTR\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForRegression\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.109,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.144,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\",\n",
      "    \"156\": \"LABEL_156\",\n",
      "    \"157\": \"LABEL_157\",\n",
      "    \"158\": \"LABEL_158\",\n",
      "    \"159\": \"LABEL_159\",\n",
      "    \"160\": \"LABEL_160\",\n",
      "    \"161\": \"LABEL_161\",\n",
      "    \"162\": \"LABEL_162\",\n",
      "    \"163\": \"LABEL_163\",\n",
      "    \"164\": \"LABEL_164\",\n",
      "    \"165\": \"LABEL_165\",\n",
      "    \"166\": \"LABEL_166\",\n",
      "    \"167\": \"LABEL_167\",\n",
      "    \"168\": \"LABEL_168\",\n",
      "    \"169\": \"LABEL_169\",\n",
      "    \"170\": \"LABEL_170\",\n",
      "    \"171\": \"LABEL_171\",\n",
      "    \"172\": \"LABEL_172\",\n",
      "    \"173\": \"LABEL_173\",\n",
      "    \"174\": \"LABEL_174\",\n",
      "    \"175\": \"LABEL_175\",\n",
      "    \"176\": \"LABEL_176\",\n",
      "    \"177\": \"LABEL_177\",\n",
      "    \"178\": \"LABEL_178\",\n",
      "    \"179\": \"LABEL_179\",\n",
      "    \"180\": \"LABEL_180\",\n",
      "    \"181\": \"LABEL_181\",\n",
      "    \"182\": \"LABEL_182\",\n",
      "    \"183\": \"LABEL_183\",\n",
      "    \"184\": \"LABEL_184\",\n",
      "    \"185\": \"LABEL_185\",\n",
      "    \"186\": \"LABEL_186\",\n",
      "    \"187\": \"LABEL_187\",\n",
      "    \"188\": \"LABEL_188\",\n",
      "    \"189\": \"LABEL_189\",\n",
      "    \"190\": \"LABEL_190\",\n",
      "    \"191\": \"LABEL_191\",\n",
      "    \"192\": \"LABEL_192\",\n",
      "    \"193\": \"LABEL_193\",\n",
      "    \"194\": \"LABEL_194\",\n",
      "    \"195\": \"LABEL_195\",\n",
      "    \"196\": \"LABEL_196\",\n",
      "    \"197\": \"LABEL_197\",\n",
      "    \"198\": \"LABEL_198\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 464,\n",
      "  \"is_gpu\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_156\": 156,\n",
      "    \"LABEL_157\": 157,\n",
      "    \"LABEL_158\": 158,\n",
      "    \"LABEL_159\": 159,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_160\": 160,\n",
      "    \"LABEL_161\": 161,\n",
      "    \"LABEL_162\": 162,\n",
      "    \"LABEL_163\": 163,\n",
      "    \"LABEL_164\": 164,\n",
      "    \"LABEL_165\": 165,\n",
      "    \"LABEL_166\": 166,\n",
      "    \"LABEL_167\": 167,\n",
      "    \"LABEL_168\": 168,\n",
      "    \"LABEL_169\": 169,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_170\": 170,\n",
      "    \"LABEL_171\": 171,\n",
      "    \"LABEL_172\": 172,\n",
      "    \"LABEL_173\": 173,\n",
      "    \"LABEL_174\": 174,\n",
      "    \"LABEL_175\": 175,\n",
      "    \"LABEL_176\": 176,\n",
      "    \"LABEL_177\": 177,\n",
      "    \"LABEL_178\": 178,\n",
      "    \"LABEL_179\": 179,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_180\": 180,\n",
      "    \"LABEL_181\": 181,\n",
      "    \"LABEL_182\": 182,\n",
      "    \"LABEL_183\": 183,\n",
      "    \"LABEL_184\": 184,\n",
      "    \"LABEL_185\": 185,\n",
      "    \"LABEL_186\": 186,\n",
      "    \"LABEL_187\": 187,\n",
      "    \"LABEL_188\": 188,\n",
      "    \"LABEL_189\": 189,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_190\": 190,\n",
      "    \"LABEL_191\": 191,\n",
      "    \"LABEL_192\": 192,\n",
      "    \"LABEL_193\": 193,\n",
      "    \"LABEL_194\": 194,\n",
      "    \"LABEL_195\": 195,\n",
      "    \"LABEL_196\": 196,\n",
      "    \"LABEL_197\": 197,\n",
      "    \"LABEL_198\": 198,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 515,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"norm_mean\": [\n",
      "    11.199569164274653,\n",
      "    -0.9728601944583675,\n",
      "    11.199595401578872,\n",
      "    0.1914454376660732,\n",
      "    0.608589373135307,\n",
      "    365.064017672,\n",
      "    342.24912812000014,\n",
      "    364.6033136038417,\n",
      "    134.06547,\n",
      "    0.004249,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    1.1861084842221647,\n",
      "    1.890967178564785,\n",
      "    2.519587985439997,\n",
      "    2.0112818114267816,\n",
      "    795.5621221754437,\n",
      "    18.14439203724506,\n",
      "    14.536240385432393,\n",
      "    15.215140271072487,\n",
      "    12.068994414289726,\n",
      "    8.453657900068215,\n",
      "    9.114162139055054,\n",
      "    6.434168605708085,\n",
      "    7.215103879809845,\n",
      "    4.436200487997215,\n",
      "    5.109730699855831,\n",
      "    3.055231525907226,\n",
      "    3.6252747118486264,\n",
      "    -2.202564923376624,\n",
      "    18.195385007867852,\n",
      "    7.9706993589944775,\n",
      "    4.5379164631837545,\n",
      "    150.95250337667272,\n",
      "    13.184208966483704,\n",
      "    8.814008658052902,\n",
      "    3.8191839078987306,\n",
      "    3.4969386790830774,\n",
      "    2.9222201316693712,\n",
      "    2.644444123964607,\n",
      "    6.408740449956927,\n",
      "    4.95314480536345,\n",
      "    2.6263770771853108,\n",
      "    2.4113616526384853,\n",
      "    26.24052195128434,\n",
      "    37.102909834641714,\n",
      "    19.89943953042712,\n",
      "    16.353848799228413,\n",
      "    15.638332143998122,\n",
      "    21.706094849865753,\n",
      "    0.28727529762970366,\n",
      "    8.054432014422119,\n",
      "    3.2648099385428853,\n",
      "    32.629006626588726,\n",
      "    16.26551059790217,\n",
      "    47.70605007162041,\n",
      "    0.0,\n",
      "    5.325837027308287,\n",
      "    9.698460925314944,\n",
      "    5.573601891254677,\n",
      "    2.581492771453006,\n",
      "    7.3124961943884665,\n",
      "    33.07539073817076,\n",
      "    10.718462271839512,\n",
      "    6.99277406210818,\n",
      "    31.684923475431933,\n",
      "    36.92162447084414,\n",
      "    1.2074202610211657,\n",
      "    5.110701506051421,\n",
      "    0.0,\n",
      "    71.04050338999998,\n",
      "    9.57750975344203,\n",
      "    10.066085526965992,\n",
      "    0.07691213090851719,\n",
      "    13.38923196114951,\n",
      "    16.862422387837878,\n",
      "    21.382953923695233,\n",
      "    15.651918121909311,\n",
      "    14.440634953378058,\n",
      "    19.13130604146014,\n",
      "    22.114944705243296,\n",
      "    8.183429061888226,\n",
      "    13.699768012021506,\n",
      "    2.1212691930096144,\n",
      "    17.474216494453906,\n",
      "    7.8467696174922725,\n",
      "    2.6683841482907034,\n",
      "    0.11868201225906093,\n",
      "    9.064881467380093,\n",
      "    2.659801877718109,\n",
      "    4.055917032498944,\n",
      "    0.259848432909807,\n",
      "    0.413963629624058,\n",
      "    25.186704,\n",
      "    1.79722,\n",
      "    5.353545,\n",
      "    0.272499,\n",
      "    0.562898,\n",
      "    0.835397,\n",
      "    1.236854,\n",
      "    0.729917,\n",
      "    1.966771,\n",
      "    4.216321,\n",
      "    1.414081,\n",
      "    6.486208,\n",
      "    5.688314,\n",
      "    0.205632,\n",
      "    0.409204,\n",
      "    0.614836,\n",
      "    2.802168,\n",
      "    2.7549044689500004,\n",
      "    97.31541557350002,\n",
      "    0.069051,\n",
      "    0.151924,\n",
      "    0.130758,\n",
      "    0.06279,\n",
      "    0.027038,\n",
      "    0.999062,\n",
      "    0.096951,\n",
      "    0.042862,\n",
      "    0.096089,\n",
      "    0.100163,\n",
      "    1.033857,\n",
      "    1.034286,\n",
      "    0.016206,\n",
      "    0.00357,\n",
      "    0.016776,\n",
      "    1.488795,\n",
      "    0.915699,\n",
      "    0.232236,\n",
      "    0.012241,\n",
      "    0.074885,\n",
      "    0.131561,\n",
      "    0.096951,\n",
      "    0.004026,\n",
      "    0.009835,\n",
      "    0.011646,\n",
      "    0.250196,\n",
      "    0.131237,\n",
      "    0.768633,\n",
      "    0.015927,\n",
      "    0.539599,\n",
      "    0.451885,\n",
      "    0.001726,\n",
      "    0.003335,\n",
      "    0.001218,\n",
      "    1.236474,\n",
      "    0.000226,\n",
      "    0.555529,\n",
      "    0.000149,\n",
      "    0.001046,\n",
      "    0.002578,\n",
      "    0.126995,\n",
      "    0.732216,\n",
      "    0.037978,\n",
      "    0.019179,\n",
      "    0.720141,\n",
      "    0.018951,\n",
      "    0.013025,\n",
      "    0.059523,\n",
      "    0.027553,\n",
      "    0.000831,\n",
      "    0.0002,\n",
      "    0.073914,\n",
      "    0.061694,\n",
      "    0.002249,\n",
      "    0.007716,\n",
      "    0.236426,\n",
      "    0.0287,\n",
      "    0.05231,\n",
      "    0.041425,\n",
      "    0.033421,\n",
      "    0.017275,\n",
      "    0.001082,\n",
      "    0.011915,\n",
      "    0.004249,\n",
      "    0.196769,\n",
      "    0.039316,\n",
      "    0.038686,\n",
      "    0.00409,\n",
      "    0.003615,\n",
      "    0.116124,\n",
      "    0.051192,\n",
      "    0.025177,\n",
      "    0.0,\n",
      "    0.161908,\n",
      "    0.315775,\n",
      "    0.087229,\n",
      "    0.079586,\n",
      "    0.023227,\n",
      "    0.005966,\n",
      "    0.007901,\n",
      "    0.050376,\n",
      "    0.000186,\n",
      "    0.065723,\n",
      "    0.380193,\n",
      "    0.051566\n",
      "  ],\n",
      "  \"norm_std\": [\n",
      "    2.9210526350021033,\n",
      "    1.5294133532822065,\n",
      "    2.9209947673330334,\n",
      "    0.21956154740898992,\n",
      "    0.22097666681598954,\n",
      "    160.48566423804579,\n",
      "    151.38170855657367,\n",
      "    160.3304390667665,\n",
      "    60.484857692625106,\n",
      "    0.181038611279414,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.24851193112366385,\n",
      "    0.317494124851492,\n",
      "    0.37175815103599535,\n",
      "    0.6098706561111424,\n",
      "    539.8195290502504,\n",
      "    8.140940922894863,\n",
      "    6.600767667198695,\n",
      "    6.700942921964325,\n",
      "    5.536318526756788,\n",
      "    4.020569431789569,\n",
      "    4.316039675035455,\n",
      "    3.229701298304296,\n",
      "    4.058753110098356,\n",
      "    2.399274478688092,\n",
      "    4.590084765547685,\n",
      "    1.8657465201411236,\n",
      "    8.197075845395899,\n",
      "    1.3989800795766576,\n",
      "    8.727770321711972,\n",
      "    4.719034225006412,\n",
      "    3.6844834579923407,\n",
      "    66.65125255607474,\n",
      "    11.022808176926917,\n",
      "    9.88512023443511,\n",
      "    5.895101555004671,\n",
      "    6.0315631910071374,\n",
      "    4.465786134186721,\n",
      "    8.73293454096314,\n",
      "    7.292192943139112,\n",
      "    5.798809757257198,\n",
      "    5.458840154330179,\n",
      "    5.34562222799046,\n",
      "    28.624753237838462,\n",
      "    22.7685485030176,\n",
      "    13.735506972569182,\n",
      "    12.75558914023291,\n",
      "    12.647297666063738,\n",
      "    16.73803715869515,\n",
      "    1.3236865505015507,\n",
      "    8.012917117258175,\n",
      "    6.328266302270954,\n",
      "    30.80439768300023,\n",
      "    14.510669158473307,\n",
      "    33.76748799216324,\n",
      "    0.0,\n",
      "    8.851153866015428,\n",
      "    8.222102882220607,\n",
      "    7.329351085680612,\n",
      "    4.87773057457412,\n",
      "    10.796349487508557,\n",
      "    24.55359833254403,\n",
      "    10.33295824604808,\n",
      "    8.986884190324291,\n",
      "    26.77991276665104,\n",
      "    29.521288543995215,\n",
      "    4.077418430037268,\n",
      "    11.23487898363004,\n",
      "    0.0,\n",
      "    50.277243284807206,\n",
      "    19.12173183245714,\n",
      "    9.819697177666312,\n",
      "    1.4201437981599128,\n",
      "    12.511435257208836,\n",
      "    14.212538029397628,\n",
      "    16.973978925056553,\n",
      "    19.21649041911615,\n",
      "    15.092240504961104,\n",
      "    19.889237093009676,\n",
      "    25.80872442073538,\n",
      "    9.254317550453825,\n",
      "    19.013243564373347,\n",
      "    3.6841568734614953,\n",
      "    17.690679185577395,\n",
      "    10.27595457263499,\n",
      "    3.3283202642652645,\n",
      "    2.8773795244438474,\n",
      "    9.228734822190495,\n",
      "    5.106296483962912,\n",
      "    4.008127533955226,\n",
      "    2.3345092198667503,\n",
      "    0.23958883840178574,\n",
      "    11.48532061063049,\n",
      "    2.0042680181777808,\n",
      "    3.411142707197923,\n",
      "    0.7103265443180337,\n",
      "    0.8009597262862117,\n",
      "    1.0630493791282618,\n",
      "    1.2495037990913607,\n",
      "    0.8592211073826755,\n",
      "    1.4909738617970663,\n",
      "    2.8049912821495706,\n",
      "    1.5692082041123125,\n",
      "    3.7188860712382157,\n",
      "    4.918753910447648,\n",
      "    0.6213838320183964,\n",
      "    0.6971589290933399,\n",
      "    0.9385507839118636,\n",
      "    1.7370945619837506,\n",
      "    2.7759468746763334,\n",
      "    43.91556441471313,\n",
      "    0.2929625321198007,\n",
      "    0.6742399816263887,\n",
      "    0.6447563579731193,\n",
      "    0.26136083143708466,\n",
      "    0.1703202147866646,\n",
      "    1.3696411924562566,\n",
      "    0.3394696140137124,\n",
      "    0.26977939457438505,\n",
      "    0.3350074869447194,\n",
      "    0.3408584597974497,\n",
      "    1.2690580420372088,\n",
      "    1.2684116362885036,\n",
      "    0.1297126917051003,\n",
      "    0.06304965563156611,\n",
      "    0.17914965229828922,\n",
      "    1.485673805113914,\n",
      "    1.1656052934139842,\n",
      "    0.5018632205797633,\n",
      "    0.15576643470973517,\n",
      "    0.2883562378800223,\n",
      "    0.3774901929558512,\n",
      "    0.3394696140137124,\n",
      "    0.07983606764988928,\n",
      "    0.10307416455777559,\n",
      "    0.11692041889415362,\n",
      "    1.0010868912132271,\n",
      "    0.7705779932112281,\n",
      "    1.157481598590082,\n",
      "    0.13507534533122212,\n",
      "    0.8359812306885952,\n",
      "    0.7600865243553028,\n",
      "    0.04757124327808961,\n",
      "    0.07183232513905516,\n",
      "    0.03513570421263404,\n",
      "    1.239225396368063,\n",
      "    0.015097985029438593,\n",
      "    1.3364349277900949,\n",
      "    0.013378265133341392,\n",
      "    0.032663541616103894,\n",
      "    0.060970137226002974,\n",
      "    0.44400840883756576,\n",
      "    1.159532265122051,\n",
      "    0.198246590935912,\n",
      "    0.1491817288215558,\n",
      "    1.28126795861232,\n",
      "    0.143114919141507,\n",
      "    0.11579880303510387,\n",
      "    0.25012811724209466,\n",
      "    0.1830406121462275,\n",
      "    0.03504726333553974,\n",
      "    0.015295758691880374,\n",
      "    0.3034514997274073,\n",
      "    0.2749689545601939,\n",
      "    0.04859983910409953,\n",
      "    0.09878498419533764,\n",
      "    0.5707110234042025,\n",
      "    0.17028898672063034,\n",
      "    0.24456026600763192,\n",
      "    0.21322057789532142,\n",
      "    0.1917343827305721,\n",
      "    0.13591391704896466,\n",
      "    0.03519702423260403,\n",
      "    0.11080182783711219,\n",
      "    0.0680510883818226,\n",
      "    0.5264724473438641,\n",
      "    0.2602735481879015,\n",
      "    0.25847912916802446,\n",
      "    0.10886360159063149,\n",
      "    0.10026934640727359,\n",
      "    0.35113436163289397,\n",
      "    0.2260341350934195,\n",
      "    0.16874580630684471,\n",
      "    0.0,\n",
      "    0.4146998571400424,\n",
      "    0.5347143492505464,\n",
      "    0.3137422508894841,\n",
      "    0.27962501103110715,\n",
      "    0.1547563582555832,\n",
      "    0.08130444916739461,\n",
      "    0.08949068223889126,\n",
      "    0.22530492534853602,\n",
      "    0.014421012861987593,\n",
      "    0.2736413019822887,\n",
      "    2.253629375384596,\n",
      "    0.22817317920167496\n",
      "  ],\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 600\n",
      "}\n",
      "\n",
      "https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /Users/manu/.cache/huggingface/transformers/tmpe_bfps63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57ede5b372046aebceaa6ad106452d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/13.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/pytorch_model.bin in cache at /Users/manu/.cache/huggingface/transformers/2aba1daad58eea9a8423ff484fc972bf293a6d16970fe2ade05a2fdbf2fe373e.7f18ba7a75e32a4b431062ac96ba246ccc4bce1826f1b4ac9deabcb01b68ef03\n",
      "creating metadata file for /Users/manu/.cache/huggingface/transformers/2aba1daad58eea9a8423ff484fc972bf293a6d16970fe2ade05a2fdbf2fe373e.7f18ba7a75e32a4b431062ac96ba246ccc4bce1826f1b4ac9deabcb01b68ef03\n",
      "loading weights file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/pytorch_model.bin from cache at /Users/manu/.cache/huggingface/transformers/2aba1daad58eea9a8423ff484fc972bf293a6d16970fe2ade05a2fdbf2fe373e.7f18ba7a75e32a4b431062ac96ba246ccc4bce1826f1b4ac9deabcb01b68ef03\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-77M-MTR were not used when initializing RobertaForSequenceClassification: ['regression.out_proj.bias', 'norm_std', 'regression.dense.bias', 'norm_mean', 'regression.out_proj.weight', 'regression.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MTR and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /Users/manu/.cache/huggingface/transformers/tmpnxx_fw96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f20078d09414571bf3fe3a532974c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/tokenizer_config.json in cache at /Users/manu/.cache/huggingface/transformers/c032d0515db96a6a63818a40ef7c966d8c98a874d4ddd30282be7c082aa5b116.390c54396032ab2c548ce80539ef985160a3a555d14825c9df9f08d7fde34e1f\n",
      "creating metadata file for /Users/manu/.cache/huggingface/transformers/c032d0515db96a6a63818a40ef7c966d8c98a874d4ddd30282be7c082aa5b116.390c54396032ab2c548ce80539ef985160a3a555d14825c9df9f08d7fde34e1f\n",
      "loading configuration file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/config.json from cache at /Users/manu/.cache/huggingface/transformers/b4baac35f554b160534b510a71ab3ab508601f5277963a9e6e634b957411009d.75548765cc29be14dbfd305bced35cb576919329f1680b8be2b4b7c2749c85bd\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MTR\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForRegression\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.109,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.144,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\",\n",
      "    \"156\": \"LABEL_156\",\n",
      "    \"157\": \"LABEL_157\",\n",
      "    \"158\": \"LABEL_158\",\n",
      "    \"159\": \"LABEL_159\",\n",
      "    \"160\": \"LABEL_160\",\n",
      "    \"161\": \"LABEL_161\",\n",
      "    \"162\": \"LABEL_162\",\n",
      "    \"163\": \"LABEL_163\",\n",
      "    \"164\": \"LABEL_164\",\n",
      "    \"165\": \"LABEL_165\",\n",
      "    \"166\": \"LABEL_166\",\n",
      "    \"167\": \"LABEL_167\",\n",
      "    \"168\": \"LABEL_168\",\n",
      "    \"169\": \"LABEL_169\",\n",
      "    \"170\": \"LABEL_170\",\n",
      "    \"171\": \"LABEL_171\",\n",
      "    \"172\": \"LABEL_172\",\n",
      "    \"173\": \"LABEL_173\",\n",
      "    \"174\": \"LABEL_174\",\n",
      "    \"175\": \"LABEL_175\",\n",
      "    \"176\": \"LABEL_176\",\n",
      "    \"177\": \"LABEL_177\",\n",
      "    \"178\": \"LABEL_178\",\n",
      "    \"179\": \"LABEL_179\",\n",
      "    \"180\": \"LABEL_180\",\n",
      "    \"181\": \"LABEL_181\",\n",
      "    \"182\": \"LABEL_182\",\n",
      "    \"183\": \"LABEL_183\",\n",
      "    \"184\": \"LABEL_184\",\n",
      "    \"185\": \"LABEL_185\",\n",
      "    \"186\": \"LABEL_186\",\n",
      "    \"187\": \"LABEL_187\",\n",
      "    \"188\": \"LABEL_188\",\n",
      "    \"189\": \"LABEL_189\",\n",
      "    \"190\": \"LABEL_190\",\n",
      "    \"191\": \"LABEL_191\",\n",
      "    \"192\": \"LABEL_192\",\n",
      "    \"193\": \"LABEL_193\",\n",
      "    \"194\": \"LABEL_194\",\n",
      "    \"195\": \"LABEL_195\",\n",
      "    \"196\": \"LABEL_196\",\n",
      "    \"197\": \"LABEL_197\",\n",
      "    \"198\": \"LABEL_198\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 464,\n",
      "  \"is_gpu\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_156\": 156,\n",
      "    \"LABEL_157\": 157,\n",
      "    \"LABEL_158\": 158,\n",
      "    \"LABEL_159\": 159,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_160\": 160,\n",
      "    \"LABEL_161\": 161,\n",
      "    \"LABEL_162\": 162,\n",
      "    \"LABEL_163\": 163,\n",
      "    \"LABEL_164\": 164,\n",
      "    \"LABEL_165\": 165,\n",
      "    \"LABEL_166\": 166,\n",
      "    \"LABEL_167\": 167,\n",
      "    \"LABEL_168\": 168,\n",
      "    \"LABEL_169\": 169,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_170\": 170,\n",
      "    \"LABEL_171\": 171,\n",
      "    \"LABEL_172\": 172,\n",
      "    \"LABEL_173\": 173,\n",
      "    \"LABEL_174\": 174,\n",
      "    \"LABEL_175\": 175,\n",
      "    \"LABEL_176\": 176,\n",
      "    \"LABEL_177\": 177,\n",
      "    \"LABEL_178\": 178,\n",
      "    \"LABEL_179\": 179,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_180\": 180,\n",
      "    \"LABEL_181\": 181,\n",
      "    \"LABEL_182\": 182,\n",
      "    \"LABEL_183\": 183,\n",
      "    \"LABEL_184\": 184,\n",
      "    \"LABEL_185\": 185,\n",
      "    \"LABEL_186\": 186,\n",
      "    \"LABEL_187\": 187,\n",
      "    \"LABEL_188\": 188,\n",
      "    \"LABEL_189\": 189,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_190\": 190,\n",
      "    \"LABEL_191\": 191,\n",
      "    \"LABEL_192\": 192,\n",
      "    \"LABEL_193\": 193,\n",
      "    \"LABEL_194\": 194,\n",
      "    \"LABEL_195\": 195,\n",
      "    \"LABEL_196\": 196,\n",
      "    \"LABEL_197\": 197,\n",
      "    \"LABEL_198\": 198,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 515,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"norm_mean\": [\n",
      "    11.199569164274653,\n",
      "    -0.9728601944583675,\n",
      "    11.199595401578872,\n",
      "    0.1914454376660732,\n",
      "    0.608589373135307,\n",
      "    365.064017672,\n",
      "    342.24912812000014,\n",
      "    364.6033136038417,\n",
      "    134.06547,\n",
      "    0.004249,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    1.1861084842221647,\n",
      "    1.890967178564785,\n",
      "    2.519587985439997,\n",
      "    2.0112818114267816,\n",
      "    795.5621221754437,\n",
      "    18.14439203724506,\n",
      "    14.536240385432393,\n",
      "    15.215140271072487,\n",
      "    12.068994414289726,\n",
      "    8.453657900068215,\n",
      "    9.114162139055054,\n",
      "    6.434168605708085,\n",
      "    7.215103879809845,\n",
      "    4.436200487997215,\n",
      "    5.109730699855831,\n",
      "    3.055231525907226,\n",
      "    3.6252747118486264,\n",
      "    -2.202564923376624,\n",
      "    18.195385007867852,\n",
      "    7.9706993589944775,\n",
      "    4.5379164631837545,\n",
      "    150.95250337667272,\n",
      "    13.184208966483704,\n",
      "    8.814008658052902,\n",
      "    3.8191839078987306,\n",
      "    3.4969386790830774,\n",
      "    2.9222201316693712,\n",
      "    2.644444123964607,\n",
      "    6.408740449956927,\n",
      "    4.95314480536345,\n",
      "    2.6263770771853108,\n",
      "    2.4113616526384853,\n",
      "    26.24052195128434,\n",
      "    37.102909834641714,\n",
      "    19.89943953042712,\n",
      "    16.353848799228413,\n",
      "    15.638332143998122,\n",
      "    21.706094849865753,\n",
      "    0.28727529762970366,\n",
      "    8.054432014422119,\n",
      "    3.2648099385428853,\n",
      "    32.629006626588726,\n",
      "    16.26551059790217,\n",
      "    47.70605007162041,\n",
      "    0.0,\n",
      "    5.325837027308287,\n",
      "    9.698460925314944,\n",
      "    5.573601891254677,\n",
      "    2.581492771453006,\n",
      "    7.3124961943884665,\n",
      "    33.07539073817076,\n",
      "    10.718462271839512,\n",
      "    6.99277406210818,\n",
      "    31.684923475431933,\n",
      "    36.92162447084414,\n",
      "    1.2074202610211657,\n",
      "    5.110701506051421,\n",
      "    0.0,\n",
      "    71.04050338999998,\n",
      "    9.57750975344203,\n",
      "    10.066085526965992,\n",
      "    0.07691213090851719,\n",
      "    13.38923196114951,\n",
      "    16.862422387837878,\n",
      "    21.382953923695233,\n",
      "    15.651918121909311,\n",
      "    14.440634953378058,\n",
      "    19.13130604146014,\n",
      "    22.114944705243296,\n",
      "    8.183429061888226,\n",
      "    13.699768012021506,\n",
      "    2.1212691930096144,\n",
      "    17.474216494453906,\n",
      "    7.8467696174922725,\n",
      "    2.6683841482907034,\n",
      "    0.11868201225906093,\n",
      "    9.064881467380093,\n",
      "    2.659801877718109,\n",
      "    4.055917032498944,\n",
      "    0.259848432909807,\n",
      "    0.413963629624058,\n",
      "    25.186704,\n",
      "    1.79722,\n",
      "    5.353545,\n",
      "    0.272499,\n",
      "    0.562898,\n",
      "    0.835397,\n",
      "    1.236854,\n",
      "    0.729917,\n",
      "    1.966771,\n",
      "    4.216321,\n",
      "    1.414081,\n",
      "    6.486208,\n",
      "    5.688314,\n",
      "    0.205632,\n",
      "    0.409204,\n",
      "    0.614836,\n",
      "    2.802168,\n",
      "    2.7549044689500004,\n",
      "    97.31541557350002,\n",
      "    0.069051,\n",
      "    0.151924,\n",
      "    0.130758,\n",
      "    0.06279,\n",
      "    0.027038,\n",
      "    0.999062,\n",
      "    0.096951,\n",
      "    0.042862,\n",
      "    0.096089,\n",
      "    0.100163,\n",
      "    1.033857,\n",
      "    1.034286,\n",
      "    0.016206,\n",
      "    0.00357,\n",
      "    0.016776,\n",
      "    1.488795,\n",
      "    0.915699,\n",
      "    0.232236,\n",
      "    0.012241,\n",
      "    0.074885,\n",
      "    0.131561,\n",
      "    0.096951,\n",
      "    0.004026,\n",
      "    0.009835,\n",
      "    0.011646,\n",
      "    0.250196,\n",
      "    0.131237,\n",
      "    0.768633,\n",
      "    0.015927,\n",
      "    0.539599,\n",
      "    0.451885,\n",
      "    0.001726,\n",
      "    0.003335,\n",
      "    0.001218,\n",
      "    1.236474,\n",
      "    0.000226,\n",
      "    0.555529,\n",
      "    0.000149,\n",
      "    0.001046,\n",
      "    0.002578,\n",
      "    0.126995,\n",
      "    0.732216,\n",
      "    0.037978,\n",
      "    0.019179,\n",
      "    0.720141,\n",
      "    0.018951,\n",
      "    0.013025,\n",
      "    0.059523,\n",
      "    0.027553,\n",
      "    0.000831,\n",
      "    0.0002,\n",
      "    0.073914,\n",
      "    0.061694,\n",
      "    0.002249,\n",
      "    0.007716,\n",
      "    0.236426,\n",
      "    0.0287,\n",
      "    0.05231,\n",
      "    0.041425,\n",
      "    0.033421,\n",
      "    0.017275,\n",
      "    0.001082,\n",
      "    0.011915,\n",
      "    0.004249,\n",
      "    0.196769,\n",
      "    0.039316,\n",
      "    0.038686,\n",
      "    0.00409,\n",
      "    0.003615,\n",
      "    0.116124,\n",
      "    0.051192,\n",
      "    0.025177,\n",
      "    0.0,\n",
      "    0.161908,\n",
      "    0.315775,\n",
      "    0.087229,\n",
      "    0.079586,\n",
      "    0.023227,\n",
      "    0.005966,\n",
      "    0.007901,\n",
      "    0.050376,\n",
      "    0.000186,\n",
      "    0.065723,\n",
      "    0.380193,\n",
      "    0.051566\n",
      "  ],\n",
      "  \"norm_std\": [\n",
      "    2.9210526350021033,\n",
      "    1.5294133532822065,\n",
      "    2.9209947673330334,\n",
      "    0.21956154740898992,\n",
      "    0.22097666681598954,\n",
      "    160.48566423804579,\n",
      "    151.38170855657367,\n",
      "    160.3304390667665,\n",
      "    60.484857692625106,\n",
      "    0.181038611279414,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.24851193112366385,\n",
      "    0.317494124851492,\n",
      "    0.37175815103599535,\n",
      "    0.6098706561111424,\n",
      "    539.8195290502504,\n",
      "    8.140940922894863,\n",
      "    6.600767667198695,\n",
      "    6.700942921964325,\n",
      "    5.536318526756788,\n",
      "    4.020569431789569,\n",
      "    4.316039675035455,\n",
      "    3.229701298304296,\n",
      "    4.058753110098356,\n",
      "    2.399274478688092,\n",
      "    4.590084765547685,\n",
      "    1.8657465201411236,\n",
      "    8.197075845395899,\n",
      "    1.3989800795766576,\n",
      "    8.727770321711972,\n",
      "    4.719034225006412,\n",
      "    3.6844834579923407,\n",
      "    66.65125255607474,\n",
      "    11.022808176926917,\n",
      "    9.88512023443511,\n",
      "    5.895101555004671,\n",
      "    6.0315631910071374,\n",
      "    4.465786134186721,\n",
      "    8.73293454096314,\n",
      "    7.292192943139112,\n",
      "    5.798809757257198,\n",
      "    5.458840154330179,\n",
      "    5.34562222799046,\n",
      "    28.624753237838462,\n",
      "    22.7685485030176,\n",
      "    13.735506972569182,\n",
      "    12.75558914023291,\n",
      "    12.647297666063738,\n",
      "    16.73803715869515,\n",
      "    1.3236865505015507,\n",
      "    8.012917117258175,\n",
      "    6.328266302270954,\n",
      "    30.80439768300023,\n",
      "    14.510669158473307,\n",
      "    33.76748799216324,\n",
      "    0.0,\n",
      "    8.851153866015428,\n",
      "    8.222102882220607,\n",
      "    7.329351085680612,\n",
      "    4.87773057457412,\n",
      "    10.796349487508557,\n",
      "    24.55359833254403,\n",
      "    10.33295824604808,\n",
      "    8.986884190324291,\n",
      "    26.77991276665104,\n",
      "    29.521288543995215,\n",
      "    4.077418430037268,\n",
      "    11.23487898363004,\n",
      "    0.0,\n",
      "    50.277243284807206,\n",
      "    19.12173183245714,\n",
      "    9.819697177666312,\n",
      "    1.4201437981599128,\n",
      "    12.511435257208836,\n",
      "    14.212538029397628,\n",
      "    16.973978925056553,\n",
      "    19.21649041911615,\n",
      "    15.092240504961104,\n",
      "    19.889237093009676,\n",
      "    25.80872442073538,\n",
      "    9.254317550453825,\n",
      "    19.013243564373347,\n",
      "    3.6841568734614953,\n",
      "    17.690679185577395,\n",
      "    10.27595457263499,\n",
      "    3.3283202642652645,\n",
      "    2.8773795244438474,\n",
      "    9.228734822190495,\n",
      "    5.106296483962912,\n",
      "    4.008127533955226,\n",
      "    2.3345092198667503,\n",
      "    0.23958883840178574,\n",
      "    11.48532061063049,\n",
      "    2.0042680181777808,\n",
      "    3.411142707197923,\n",
      "    0.7103265443180337,\n",
      "    0.8009597262862117,\n",
      "    1.0630493791282618,\n",
      "    1.2495037990913607,\n",
      "    0.8592211073826755,\n",
      "    1.4909738617970663,\n",
      "    2.8049912821495706,\n",
      "    1.5692082041123125,\n",
      "    3.7188860712382157,\n",
      "    4.918753910447648,\n",
      "    0.6213838320183964,\n",
      "    0.6971589290933399,\n",
      "    0.9385507839118636,\n",
      "    1.7370945619837506,\n",
      "    2.7759468746763334,\n",
      "    43.91556441471313,\n",
      "    0.2929625321198007,\n",
      "    0.6742399816263887,\n",
      "    0.6447563579731193,\n",
      "    0.26136083143708466,\n",
      "    0.1703202147866646,\n",
      "    1.3696411924562566,\n",
      "    0.3394696140137124,\n",
      "    0.26977939457438505,\n",
      "    0.3350074869447194,\n",
      "    0.3408584597974497,\n",
      "    1.2690580420372088,\n",
      "    1.2684116362885036,\n",
      "    0.1297126917051003,\n",
      "    0.06304965563156611,\n",
      "    0.17914965229828922,\n",
      "    1.485673805113914,\n",
      "    1.1656052934139842,\n",
      "    0.5018632205797633,\n",
      "    0.15576643470973517,\n",
      "    0.2883562378800223,\n",
      "    0.3774901929558512,\n",
      "    0.3394696140137124,\n",
      "    0.07983606764988928,\n",
      "    0.10307416455777559,\n",
      "    0.11692041889415362,\n",
      "    1.0010868912132271,\n",
      "    0.7705779932112281,\n",
      "    1.157481598590082,\n",
      "    0.13507534533122212,\n",
      "    0.8359812306885952,\n",
      "    0.7600865243553028,\n",
      "    0.04757124327808961,\n",
      "    0.07183232513905516,\n",
      "    0.03513570421263404,\n",
      "    1.239225396368063,\n",
      "    0.015097985029438593,\n",
      "    1.3364349277900949,\n",
      "    0.013378265133341392,\n",
      "    0.032663541616103894,\n",
      "    0.060970137226002974,\n",
      "    0.44400840883756576,\n",
      "    1.159532265122051,\n",
      "    0.198246590935912,\n",
      "    0.1491817288215558,\n",
      "    1.28126795861232,\n",
      "    0.143114919141507,\n",
      "    0.11579880303510387,\n",
      "    0.25012811724209466,\n",
      "    0.1830406121462275,\n",
      "    0.03504726333553974,\n",
      "    0.015295758691880374,\n",
      "    0.3034514997274073,\n",
      "    0.2749689545601939,\n",
      "    0.04859983910409953,\n",
      "    0.09878498419533764,\n",
      "    0.5707110234042025,\n",
      "    0.17028898672063034,\n",
      "    0.24456026600763192,\n",
      "    0.21322057789532142,\n",
      "    0.1917343827305721,\n",
      "    0.13591391704896466,\n",
      "    0.03519702423260403,\n",
      "    0.11080182783711219,\n",
      "    0.0680510883818226,\n",
      "    0.5264724473438641,\n",
      "    0.2602735481879015,\n",
      "    0.25847912916802446,\n",
      "    0.10886360159063149,\n",
      "    0.10026934640727359,\n",
      "    0.35113436163289397,\n",
      "    0.2260341350934195,\n",
      "    0.16874580630684471,\n",
      "    0.0,\n",
      "    0.4146998571400424,\n",
      "    0.5347143492505464,\n",
      "    0.3137422508894841,\n",
      "    0.27962501103110715,\n",
      "    0.1547563582555832,\n",
      "    0.08130444916739461,\n",
      "    0.08949068223889126,\n",
      "    0.22530492534853602,\n",
      "    0.014421012861987593,\n",
      "    0.2736413019822887,\n",
      "    2.253629375384596,\n",
      "    0.22817317920167496\n",
      "  ],\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 600\n",
      "}\n",
      "\n",
      "https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /Users/manu/.cache/huggingface/transformers/tmpicpqewot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294cc365e0a94a14b73e2370685e3497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/6.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/vocab.json in cache at /Users/manu/.cache/huggingface/transformers/48d1a98074d4e2b3eb2476016213b0e08b724ba2f0173763392eb0cfcb68f7f9.5468158f181199f93da5588d09de73fba27535f90fd445a5a176e3fc5f04919f\n",
      "creating metadata file for /Users/manu/.cache/huggingface/transformers/48d1a98074d4e2b3eb2476016213b0e08b724ba2f0173763392eb0cfcb68f7f9.5468158f181199f93da5588d09de73fba27535f90fd445a5a176e3fc5f04919f\n",
      "https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /Users/manu/.cache/huggingface/transformers/tmpb9rtxnyj\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b1681431d24a35b7487faa989736fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/merges.txt in cache at /Users/manu/.cache/huggingface/transformers/4995467a5d1ecedd5790567393d7cb26ed58fa7f77b98b3f67ce8806a696cd67.90e27582e69797144e6cc9a63c829cad1addb24967933d413b42b370f1abf12a\n",
      "creating metadata file for /Users/manu/.cache/huggingface/transformers/4995467a5d1ecedd5790567393d7cb26ed58fa7f77b98b3f67ce8806a696cd67.90e27582e69797144e6cc9a63c829cad1addb24967933d413b42b370f1abf12a\n",
      "https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /Users/manu/.cache/huggingface/transformers/tmpniuny_yw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bcffe9385542ab9aab8fc05269cdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/8.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/tokenizer.json in cache at /Users/manu/.cache/huggingface/transformers/0a8cbdb33e4c81aa5d36f49ae7a74b68fbfd0c8a119614cfeb6117776feb7016.bcf9c4ad82082f086180ae9806383a789257bf6ccbb246ae63003c741db58ba6\n",
      "creating metadata file for /Users/manu/.cache/huggingface/transformers/0a8cbdb33e4c81aa5d36f49ae7a74b68fbfd0c8a119614cfeb6117776feb7016.bcf9c4ad82082f086180ae9806383a789257bf6ccbb246ae63003c741db58ba6\n",
      "https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/added_tokens.json not found in cache or force_download set to True, downloading to /Users/manu/.cache/huggingface/transformers/tmpasy05fmw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafe9db24fa54c4fa941704c56b20db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/added_tokens.json in cache at /Users/manu/.cache/huggingface/transformers/2be3d78817c28abd93b888b62c071e5b6722e470634c3ef1aca795d82b8b945f.c955c51d28400e68b7aaede5ce5dca96c67912c030fb145bc1f142003a4bf11b\n",
      "creating metadata file for /Users/manu/.cache/huggingface/transformers/2be3d78817c28abd93b888b62c071e5b6722e470634c3ef1aca795d82b8b945f.c955c51d28400e68b7aaede5ce5dca96c67912c030fb145bc1f142003a4bf11b\n",
      "https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /Users/manu/.cache/huggingface/transformers/tmpvs57b7ep\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ead2196fc34eb894e26b5581cfc5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading special_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/special_tokens_map.json in cache at /Users/manu/.cache/huggingface/transformers/57eaf46c6b11bba3bff16dda699a2cad84a8216ebc548c9f17d476cb89334807.106100a43b7ce7464c0b8915979ee7c7f36043b18c0119ed38b6f7675f24149c\n",
      "creating metadata file for /Users/manu/.cache/huggingface/transformers/57eaf46c6b11bba3bff16dda699a2cad84a8216ebc548c9f17d476cb89334807.106100a43b7ce7464c0b8915979ee7c7f36043b18c0119ed38b6f7675f24149c\n",
      "loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/vocab.json from cache at /Users/manu/.cache/huggingface/transformers/48d1a98074d4e2b3eb2476016213b0e08b724ba2f0173763392eb0cfcb68f7f9.5468158f181199f93da5588d09de73fba27535f90fd445a5a176e3fc5f04919f\n",
      "loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/merges.txt from cache at /Users/manu/.cache/huggingface/transformers/4995467a5d1ecedd5790567393d7cb26ed58fa7f77b98b3f67ce8806a696cd67.90e27582e69797144e6cc9a63c829cad1addb24967933d413b42b370f1abf12a\n",
      "loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/tokenizer.json from cache at /Users/manu/.cache/huggingface/transformers/0a8cbdb33e4c81aa5d36f49ae7a74b68fbfd0c8a119614cfeb6117776feb7016.bcf9c4ad82082f086180ae9806383a789257bf6ccbb246ae63003c741db58ba6\n",
      "loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/added_tokens.json from cache at /Users/manu/.cache/huggingface/transformers/2be3d78817c28abd93b888b62c071e5b6722e470634c3ef1aca795d82b8b945f.c955c51d28400e68b7aaede5ce5dca96c67912c030fb145bc1f142003a4bf11b\n",
      "loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/special_tokens_map.json from cache at /Users/manu/.cache/huggingface/transformers/57eaf46c6b11bba3bff16dda699a2cad84a8216ebc548c9f17d476cb89334807.106100a43b7ce7464c0b8915979ee7c7f36043b18c0119ed38b6f7675f24149c\n",
      "loading file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/tokenizer_config.json from cache at /Users/manu/.cache/huggingface/transformers/c032d0515db96a6a63818a40ef7c966d8c98a874d4ddd30282be7c082aa5b116.390c54396032ab2c548ce80539ef985160a3a555d14825c9df9f08d7fde34e1f\n",
      "loading configuration file https://huggingface.co/DeepChem/ChemBERTa-77M-MTR/resolve/main/config.json from cache at /Users/manu/.cache/huggingface/transformers/b4baac35f554b160534b510a71ab3ab508601f5277963a9e6e634b957411009d.75548765cc29be14dbfd305bced35cb576919329f1680b8be2b4b7c2749c85bd\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"DeepChem/ChemBERTa-77M-MTR\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForRegression\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.109,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.144,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\",\n",
      "    \"156\": \"LABEL_156\",\n",
      "    \"157\": \"LABEL_157\",\n",
      "    \"158\": \"LABEL_158\",\n",
      "    \"159\": \"LABEL_159\",\n",
      "    \"160\": \"LABEL_160\",\n",
      "    \"161\": \"LABEL_161\",\n",
      "    \"162\": \"LABEL_162\",\n",
      "    \"163\": \"LABEL_163\",\n",
      "    \"164\": \"LABEL_164\",\n",
      "    \"165\": \"LABEL_165\",\n",
      "    \"166\": \"LABEL_166\",\n",
      "    \"167\": \"LABEL_167\",\n",
      "    \"168\": \"LABEL_168\",\n",
      "    \"169\": \"LABEL_169\",\n",
      "    \"170\": \"LABEL_170\",\n",
      "    \"171\": \"LABEL_171\",\n",
      "    \"172\": \"LABEL_172\",\n",
      "    \"173\": \"LABEL_173\",\n",
      "    \"174\": \"LABEL_174\",\n",
      "    \"175\": \"LABEL_175\",\n",
      "    \"176\": \"LABEL_176\",\n",
      "    \"177\": \"LABEL_177\",\n",
      "    \"178\": \"LABEL_178\",\n",
      "    \"179\": \"LABEL_179\",\n",
      "    \"180\": \"LABEL_180\",\n",
      "    \"181\": \"LABEL_181\",\n",
      "    \"182\": \"LABEL_182\",\n",
      "    \"183\": \"LABEL_183\",\n",
      "    \"184\": \"LABEL_184\",\n",
      "    \"185\": \"LABEL_185\",\n",
      "    \"186\": \"LABEL_186\",\n",
      "    \"187\": \"LABEL_187\",\n",
      "    \"188\": \"LABEL_188\",\n",
      "    \"189\": \"LABEL_189\",\n",
      "    \"190\": \"LABEL_190\",\n",
      "    \"191\": \"LABEL_191\",\n",
      "    \"192\": \"LABEL_192\",\n",
      "    \"193\": \"LABEL_193\",\n",
      "    \"194\": \"LABEL_194\",\n",
      "    \"195\": \"LABEL_195\",\n",
      "    \"196\": \"LABEL_196\",\n",
      "    \"197\": \"LABEL_197\",\n",
      "    \"198\": \"LABEL_198\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 464,\n",
      "  \"is_gpu\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_156\": 156,\n",
      "    \"LABEL_157\": 157,\n",
      "    \"LABEL_158\": 158,\n",
      "    \"LABEL_159\": 159,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_160\": 160,\n",
      "    \"LABEL_161\": 161,\n",
      "    \"LABEL_162\": 162,\n",
      "    \"LABEL_163\": 163,\n",
      "    \"LABEL_164\": 164,\n",
      "    \"LABEL_165\": 165,\n",
      "    \"LABEL_166\": 166,\n",
      "    \"LABEL_167\": 167,\n",
      "    \"LABEL_168\": 168,\n",
      "    \"LABEL_169\": 169,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_170\": 170,\n",
      "    \"LABEL_171\": 171,\n",
      "    \"LABEL_172\": 172,\n",
      "    \"LABEL_173\": 173,\n",
      "    \"LABEL_174\": 174,\n",
      "    \"LABEL_175\": 175,\n",
      "    \"LABEL_176\": 176,\n",
      "    \"LABEL_177\": 177,\n",
      "    \"LABEL_178\": 178,\n",
      "    \"LABEL_179\": 179,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_180\": 180,\n",
      "    \"LABEL_181\": 181,\n",
      "    \"LABEL_182\": 182,\n",
      "    \"LABEL_183\": 183,\n",
      "    \"LABEL_184\": 184,\n",
      "    \"LABEL_185\": 185,\n",
      "    \"LABEL_186\": 186,\n",
      "    \"LABEL_187\": 187,\n",
      "    \"LABEL_188\": 188,\n",
      "    \"LABEL_189\": 189,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_190\": 190,\n",
      "    \"LABEL_191\": 191,\n",
      "    \"LABEL_192\": 192,\n",
      "    \"LABEL_193\": 193,\n",
      "    \"LABEL_194\": 194,\n",
      "    \"LABEL_195\": 195,\n",
      "    \"LABEL_196\": 196,\n",
      "    \"LABEL_197\": 197,\n",
      "    \"LABEL_198\": 198,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 515,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"norm_mean\": [\n",
      "    11.199569164274653,\n",
      "    -0.9728601944583675,\n",
      "    11.199595401578872,\n",
      "    0.1914454376660732,\n",
      "    0.608589373135307,\n",
      "    365.064017672,\n",
      "    342.24912812000014,\n",
      "    364.6033136038417,\n",
      "    134.06547,\n",
      "    0.004249,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    1.1861084842221647,\n",
      "    1.890967178564785,\n",
      "    2.519587985439997,\n",
      "    2.0112818114267816,\n",
      "    795.5621221754437,\n",
      "    18.14439203724506,\n",
      "    14.536240385432393,\n",
      "    15.215140271072487,\n",
      "    12.068994414289726,\n",
      "    8.453657900068215,\n",
      "    9.114162139055054,\n",
      "    6.434168605708085,\n",
      "    7.215103879809845,\n",
      "    4.436200487997215,\n",
      "    5.109730699855831,\n",
      "    3.055231525907226,\n",
      "    3.6252747118486264,\n",
      "    -2.202564923376624,\n",
      "    18.195385007867852,\n",
      "    7.9706993589944775,\n",
      "    4.5379164631837545,\n",
      "    150.95250337667272,\n",
      "    13.184208966483704,\n",
      "    8.814008658052902,\n",
      "    3.8191839078987306,\n",
      "    3.4969386790830774,\n",
      "    2.9222201316693712,\n",
      "    2.644444123964607,\n",
      "    6.408740449956927,\n",
      "    4.95314480536345,\n",
      "    2.6263770771853108,\n",
      "    2.4113616526384853,\n",
      "    26.24052195128434,\n",
      "    37.102909834641714,\n",
      "    19.89943953042712,\n",
      "    16.353848799228413,\n",
      "    15.638332143998122,\n",
      "    21.706094849865753,\n",
      "    0.28727529762970366,\n",
      "    8.054432014422119,\n",
      "    3.2648099385428853,\n",
      "    32.629006626588726,\n",
      "    16.26551059790217,\n",
      "    47.70605007162041,\n",
      "    0.0,\n",
      "    5.325837027308287,\n",
      "    9.698460925314944,\n",
      "    5.573601891254677,\n",
      "    2.581492771453006,\n",
      "    7.3124961943884665,\n",
      "    33.07539073817076,\n",
      "    10.718462271839512,\n",
      "    6.99277406210818,\n",
      "    31.684923475431933,\n",
      "    36.92162447084414,\n",
      "    1.2074202610211657,\n",
      "    5.110701506051421,\n",
      "    0.0,\n",
      "    71.04050338999998,\n",
      "    9.57750975344203,\n",
      "    10.066085526965992,\n",
      "    0.07691213090851719,\n",
      "    13.38923196114951,\n",
      "    16.862422387837878,\n",
      "    21.382953923695233,\n",
      "    15.651918121909311,\n",
      "    14.440634953378058,\n",
      "    19.13130604146014,\n",
      "    22.114944705243296,\n",
      "    8.183429061888226,\n",
      "    13.699768012021506,\n",
      "    2.1212691930096144,\n",
      "    17.474216494453906,\n",
      "    7.8467696174922725,\n",
      "    2.6683841482907034,\n",
      "    0.11868201225906093,\n",
      "    9.064881467380093,\n",
      "    2.659801877718109,\n",
      "    4.055917032498944,\n",
      "    0.259848432909807,\n",
      "    0.413963629624058,\n",
      "    25.186704,\n",
      "    1.79722,\n",
      "    5.353545,\n",
      "    0.272499,\n",
      "    0.562898,\n",
      "    0.835397,\n",
      "    1.236854,\n",
      "    0.729917,\n",
      "    1.966771,\n",
      "    4.216321,\n",
      "    1.414081,\n",
      "    6.486208,\n",
      "    5.688314,\n",
      "    0.205632,\n",
      "    0.409204,\n",
      "    0.614836,\n",
      "    2.802168,\n",
      "    2.7549044689500004,\n",
      "    97.31541557350002,\n",
      "    0.069051,\n",
      "    0.151924,\n",
      "    0.130758,\n",
      "    0.06279,\n",
      "    0.027038,\n",
      "    0.999062,\n",
      "    0.096951,\n",
      "    0.042862,\n",
      "    0.096089,\n",
      "    0.100163,\n",
      "    1.033857,\n",
      "    1.034286,\n",
      "    0.016206,\n",
      "    0.00357,\n",
      "    0.016776,\n",
      "    1.488795,\n",
      "    0.915699,\n",
      "    0.232236,\n",
      "    0.012241,\n",
      "    0.074885,\n",
      "    0.131561,\n",
      "    0.096951,\n",
      "    0.004026,\n",
      "    0.009835,\n",
      "    0.011646,\n",
      "    0.250196,\n",
      "    0.131237,\n",
      "    0.768633,\n",
      "    0.015927,\n",
      "    0.539599,\n",
      "    0.451885,\n",
      "    0.001726,\n",
      "    0.003335,\n",
      "    0.001218,\n",
      "    1.236474,\n",
      "    0.000226,\n",
      "    0.555529,\n",
      "    0.000149,\n",
      "    0.001046,\n",
      "    0.002578,\n",
      "    0.126995,\n",
      "    0.732216,\n",
      "    0.037978,\n",
      "    0.019179,\n",
      "    0.720141,\n",
      "    0.018951,\n",
      "    0.013025,\n",
      "    0.059523,\n",
      "    0.027553,\n",
      "    0.000831,\n",
      "    0.0002,\n",
      "    0.073914,\n",
      "    0.061694,\n",
      "    0.002249,\n",
      "    0.007716,\n",
      "    0.236426,\n",
      "    0.0287,\n",
      "    0.05231,\n",
      "    0.041425,\n",
      "    0.033421,\n",
      "    0.017275,\n",
      "    0.001082,\n",
      "    0.011915,\n",
      "    0.004249,\n",
      "    0.196769,\n",
      "    0.039316,\n",
      "    0.038686,\n",
      "    0.00409,\n",
      "    0.003615,\n",
      "    0.116124,\n",
      "    0.051192,\n",
      "    0.025177,\n",
      "    0.0,\n",
      "    0.161908,\n",
      "    0.315775,\n",
      "    0.087229,\n",
      "    0.079586,\n",
      "    0.023227,\n",
      "    0.005966,\n",
      "    0.007901,\n",
      "    0.050376,\n",
      "    0.000186,\n",
      "    0.065723,\n",
      "    0.380193,\n",
      "    0.051566\n",
      "  ],\n",
      "  \"norm_std\": [\n",
      "    2.9210526350021033,\n",
      "    1.5294133532822065,\n",
      "    2.9209947673330334,\n",
      "    0.21956154740898992,\n",
      "    0.22097666681598954,\n",
      "    160.48566423804579,\n",
      "    151.38170855657367,\n",
      "    160.3304390667665,\n",
      "    60.484857692625106,\n",
      "    0.181038611279414,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.0,\n",
      "    0.24851193112366385,\n",
      "    0.317494124851492,\n",
      "    0.37175815103599535,\n",
      "    0.6098706561111424,\n",
      "    539.8195290502504,\n",
      "    8.140940922894863,\n",
      "    6.600767667198695,\n",
      "    6.700942921964325,\n",
      "    5.536318526756788,\n",
      "    4.020569431789569,\n",
      "    4.316039675035455,\n",
      "    3.229701298304296,\n",
      "    4.058753110098356,\n",
      "    2.399274478688092,\n",
      "    4.590084765547685,\n",
      "    1.8657465201411236,\n",
      "    8.197075845395899,\n",
      "    1.3989800795766576,\n",
      "    8.727770321711972,\n",
      "    4.719034225006412,\n",
      "    3.6844834579923407,\n",
      "    66.65125255607474,\n",
      "    11.022808176926917,\n",
      "    9.88512023443511,\n",
      "    5.895101555004671,\n",
      "    6.0315631910071374,\n",
      "    4.465786134186721,\n",
      "    8.73293454096314,\n",
      "    7.292192943139112,\n",
      "    5.798809757257198,\n",
      "    5.458840154330179,\n",
      "    5.34562222799046,\n",
      "    28.624753237838462,\n",
      "    22.7685485030176,\n",
      "    13.735506972569182,\n",
      "    12.75558914023291,\n",
      "    12.647297666063738,\n",
      "    16.73803715869515,\n",
      "    1.3236865505015507,\n",
      "    8.012917117258175,\n",
      "    6.328266302270954,\n",
      "    30.80439768300023,\n",
      "    14.510669158473307,\n",
      "    33.76748799216324,\n",
      "    0.0,\n",
      "    8.851153866015428,\n",
      "    8.222102882220607,\n",
      "    7.329351085680612,\n",
      "    4.87773057457412,\n",
      "    10.796349487508557,\n",
      "    24.55359833254403,\n",
      "    10.33295824604808,\n",
      "    8.986884190324291,\n",
      "    26.77991276665104,\n",
      "    29.521288543995215,\n",
      "    4.077418430037268,\n",
      "    11.23487898363004,\n",
      "    0.0,\n",
      "    50.277243284807206,\n",
      "    19.12173183245714,\n",
      "    9.819697177666312,\n",
      "    1.4201437981599128,\n",
      "    12.511435257208836,\n",
      "    14.212538029397628,\n",
      "    16.973978925056553,\n",
      "    19.21649041911615,\n",
      "    15.092240504961104,\n",
      "    19.889237093009676,\n",
      "    25.80872442073538,\n",
      "    9.254317550453825,\n",
      "    19.013243564373347,\n",
      "    3.6841568734614953,\n",
      "    17.690679185577395,\n",
      "    10.27595457263499,\n",
      "    3.3283202642652645,\n",
      "    2.8773795244438474,\n",
      "    9.228734822190495,\n",
      "    5.106296483962912,\n",
      "    4.008127533955226,\n",
      "    2.3345092198667503,\n",
      "    0.23958883840178574,\n",
      "    11.48532061063049,\n",
      "    2.0042680181777808,\n",
      "    3.411142707197923,\n",
      "    0.7103265443180337,\n",
      "    0.8009597262862117,\n",
      "    1.0630493791282618,\n",
      "    1.2495037990913607,\n",
      "    0.8592211073826755,\n",
      "    1.4909738617970663,\n",
      "    2.8049912821495706,\n",
      "    1.5692082041123125,\n",
      "    3.7188860712382157,\n",
      "    4.918753910447648,\n",
      "    0.6213838320183964,\n",
      "    0.6971589290933399,\n",
      "    0.9385507839118636,\n",
      "    1.7370945619837506,\n",
      "    2.7759468746763334,\n",
      "    43.91556441471313,\n",
      "    0.2929625321198007,\n",
      "    0.6742399816263887,\n",
      "    0.6447563579731193,\n",
      "    0.26136083143708466,\n",
      "    0.1703202147866646,\n",
      "    1.3696411924562566,\n",
      "    0.3394696140137124,\n",
      "    0.26977939457438505,\n",
      "    0.3350074869447194,\n",
      "    0.3408584597974497,\n",
      "    1.2690580420372088,\n",
      "    1.2684116362885036,\n",
      "    0.1297126917051003,\n",
      "    0.06304965563156611,\n",
      "    0.17914965229828922,\n",
      "    1.485673805113914,\n",
      "    1.1656052934139842,\n",
      "    0.5018632205797633,\n",
      "    0.15576643470973517,\n",
      "    0.2883562378800223,\n",
      "    0.3774901929558512,\n",
      "    0.3394696140137124,\n",
      "    0.07983606764988928,\n",
      "    0.10307416455777559,\n",
      "    0.11692041889415362,\n",
      "    1.0010868912132271,\n",
      "    0.7705779932112281,\n",
      "    1.157481598590082,\n",
      "    0.13507534533122212,\n",
      "    0.8359812306885952,\n",
      "    0.7600865243553028,\n",
      "    0.04757124327808961,\n",
      "    0.07183232513905516,\n",
      "    0.03513570421263404,\n",
      "    1.239225396368063,\n",
      "    0.015097985029438593,\n",
      "    1.3364349277900949,\n",
      "    0.013378265133341392,\n",
      "    0.032663541616103894,\n",
      "    0.060970137226002974,\n",
      "    0.44400840883756576,\n",
      "    1.159532265122051,\n",
      "    0.198246590935912,\n",
      "    0.1491817288215558,\n",
      "    1.28126795861232,\n",
      "    0.143114919141507,\n",
      "    0.11579880303510387,\n",
      "    0.25012811724209466,\n",
      "    0.1830406121462275,\n",
      "    0.03504726333553974,\n",
      "    0.015295758691880374,\n",
      "    0.3034514997274073,\n",
      "    0.2749689545601939,\n",
      "    0.04859983910409953,\n",
      "    0.09878498419533764,\n",
      "    0.5707110234042025,\n",
      "    0.17028898672063034,\n",
      "    0.24456026600763192,\n",
      "    0.21322057789532142,\n",
      "    0.1917343827305721,\n",
      "    0.13591391704896466,\n",
      "    0.03519702423260403,\n",
      "    0.11080182783711219,\n",
      "    0.0680510883818226,\n",
      "    0.5264724473438641,\n",
      "    0.2602735481879015,\n",
      "    0.25847912916802446,\n",
      "    0.10886360159063149,\n",
      "    0.10026934640727359,\n",
      "    0.35113436163289397,\n",
      "    0.2260341350934195,\n",
      "    0.16874580630684471,\n",
      "    0.0,\n",
      "    0.4146998571400424,\n",
      "    0.5347143492505464,\n",
      "    0.3137422508894841,\n",
      "    0.27962501103110715,\n",
      "    0.1547563582555832,\n",
      "    0.08130444916739461,\n",
      "    0.08949068223889126,\n",
      "    0.22530492534853602,\n",
      "    0.014421012861987593,\n",
      "    0.2736413019822887,\n",
      "    2.253629375384596,\n",
      "    0.22817317920167496\n",
      "  ],\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 600\n",
      "}\n",
      "\n",
      "Configuration saved in /var/folders/zt/ck4vrp4n4vsb0v16tnlh9h9m0000gn/T/tmpol8n80qq/config.json\n",
      "Model weights saved in /var/folders/zt/ck4vrp4n4vsb0v16tnlh9h9m0000gn/T/tmpol8n80qq/pytorch_model.bin\n",
      "tokenizer config file saved in /var/folders/zt/ck4vrp4n4vsb0v16tnlh9h9m0000gn/T/tmpol8n80qq/tokenizer_config.json\n",
      "Special tokens file saved in /var/folders/zt/ck4vrp4n4vsb0v16tnlh9h9m0000gn/T/tmpol8n80qq/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa023f5aab214adf8af4864a8c31e1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 00:44:20.591 | INFO     | molfeat.trans.pretrained.hf_transformers:save:49 - Model saved to gs://molfeat-store-dev/artifacts/huggingface/DeepChem-ChemBERTa-77M-MTR/0/model.save\n",
      "2023-02-03 00:44:22.068 | INFO     | molfeat.store.modelstore:register:124 - Successfuly registered model DeepChem-ChemBERTa-77M-MTR !\n"
     ]
    }
   ],
   "source": [
    "# attempt to register the model\n",
    "model_mtr = HFModel.register_pretrained(\"DeepChem/ChemBERTa-77M-MTR\", \"DeepChem/ChemBERTa-77M-MTR\", chemberta_77_mtr_card, model_class=AutoModelForSequenceClassification)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChemGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemgpt_4M = ModelInfo(\n",
    "    name = \"ChemGPT-4.7M\",\n",
    "    inputs = \"selfies\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"ChemGPT (4.7M params) is a transformer model for generative molecular modeling, which was pretrained on the PubChem10M dataset.\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"ChemGPT\", 'huggingface', \"transformers\", \"GPTNeo\", \"PubChem\", \"selfies\", \"small\"],\n",
    "    authors= ['Nathan Frey',\n",
    "        'Ryan Soklaski',\n",
    "        'Simon Axelrod',\n",
    "        'Siddharth Samsi',\n",
    "        'Rafael Gomez-Bombarelli',\n",
    "        'Connor Coley',\n",
    "        'Vijay Gadepally'\n",
    "    ],\n",
    "    reference = \"10.26434/chemrxiv-2022-3s512\" \n",
    ")\n",
    "\n",
    "\n",
    "chemgpt_1B = ModelInfo(\n",
    "    name = \"ChemGPT-1.2B\",\n",
    "    inputs = \"selfies\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"ChemGPT (1.2B params) is a transformer model for generative molecular modeling, which was pretrained on the PubChem10M dataset.\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"ChemGPT\", 'huggingface', \"transformers\", \"GPTNeo\", \"PubChem\", \"selfies\", \"huge\"],\n",
    "    authors= ['Nathan Frey',\n",
    "        'Ryan Soklaski',\n",
    "        'Simon Axelrod',\n",
    "        'Siddharth Samsi',\n",
    "        'Rafael Gomez-Bombarelli',\n",
    "        'Connor Coley',\n",
    "        'Vijay Gadepally'\n",
    "    ],\n",
    "    reference = \"10.26434/chemrxiv-2022-3s512\" \n",
    ")\n",
    "\n",
    "chemgpt_19M = ModelInfo(\n",
    "    name = \"ChemGPT-1.2B\",\n",
    "    inputs = \"selfies\",\n",
    "    type=\"pretrained\",\n",
    "    group=\"huggingface\",\n",
    "    version=0,\n",
    "    submitter=\"Datamol\",\n",
    "    description=\"ChemGPT (1.2B params) is a transformers model for generative molecular modeling, which was pretrained on the PubChem10M dataset.\",\n",
    "    representation=\"line-notation\",\n",
    "    require_3D=False,\n",
    "    tags = [\"ChemGPT\", 'huggingface', \"transformers\", \"GPTNeo\", \"PubChem\", \"selfies\", \"large\"],\n",
    "    authors= ['Nathan Frey',\n",
    "        'Ryan Soklaski',\n",
    "        'Simon Axelrod',\n",
    "        'Siddharth Samsi',\n",
    "        'Rafael Gomez-Bombarelli',\n",
    "        'Connor Coley',\n",
    "        'Vijay Gadepally'\n",
    "    ],\n",
    "    reference = \"10.26434/chemrxiv-2022-3s512\" \n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ChemGPT, we need to patch the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_4M = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-4.7M\")\n",
    "tokenizer_1B = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-1.2B\")\n",
    "tokenizer_19M = AutoTokenizer.from_pretrained(\"ncfrey/ChemGPT-19M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_tokenizer(tokenizer):\n",
    "    # unk\n",
    "    tokenizer.unk_token = \"[UNK]\"\n",
    "    tokenizer.unk_token_id = tokenizer.vocab.get(\"[UNK]\")\n",
    "\n",
    "    # cls\n",
    "    tokenizer.cls_token = \"[CLS]\"\n",
    "    tokenizer.cls_token_id = tokenizer.vocab.get(\"[CLS]\")\n",
    "\n",
    "    # bos\n",
    "    tokenizer.bos_token = \"[CLS]\"\n",
    "    tokenizer.bos_token_id = tokenizer.vocab.get(\"[CLS]\")\n",
    "    \n",
    "    # sep\n",
    "    tokenizer.sep_token = \"[SEP]\"\n",
    "    tokenizer.sep_token_id = tokenizer.vocab.get(\"[SEP]\")\n",
    "    \n",
    "    # EN: My guess is that the EOS token is the one that is wrong\n",
    "    # eos\n",
    "    tokenizer.eos_token = \"[SEP]\"\n",
    "    tokenizer.eos_token_id = tokenizer.vocab.get(\"[SEP]\")\n",
    "    \n",
    "    # mask\n",
    "    tokenizer.mask_token = \"[MASK]\"\n",
    "    tokenizer.mask_token_id = tokenizer.vocab.get(\"[MASK]\")\n",
    "    \n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_4M = patch_tokenizer(tokenizer_4M)\n",
    "tokenizer_19M = patch_tokenizer(tokenizer_19M)\n",
    "tokenizer_1B = patch_tokenizer(tokenizer_1B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model_4M = AutoModelForCausalLM.from_pretrained(\"ncfrey/ChemGPT-4.7M\")\n",
    "#model_1B = AutoModelForCausalLM.from_pretrained(\"ncfrey/ChemGPT-1.2B\")\n",
    "model_19M = AutoModelForCausalLM.from_pretrained(\"ncfrey/ChemGPT-19M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def patch_model_from_tokenizer(model, tokenizer):\n",
    "    \"\"\"This is copied from Factory.patch_hgf_config_from_tokenizer\"\"\"\n",
    "    config = model.config\n",
    "\n",
    "    conf_dict_data = {}\n",
    "    for conf_key in [\n",
    "        \"bos_token_id\",\n",
    "        \"eos_token_id\",\n",
    "        \"pad_token_id\",\n",
    "        \"unk_token_id\",\n",
    "        \"mask_token_id\",\n",
    "        \"sep_token_id\",\n",
    "        \"cls_token_id\",\n",
    "    ]:\n",
    "        if hasattr(config, conf_key):\n",
    "            conf_dict_data[conf_key] = getattr(tokenizer, conf_key)\n",
    "    for conf_key in [\"forced_eos_token_id\", \"decoder_start_token_id\"]:\n",
    "        if hasattr(config, conf_key):\n",
    "            conf_dict_data[conf_key] = tokenizer.eos_token_id\n",
    "    #conf_dict_data[\"vocab_size\"] = len(tokenizer)\n",
    "    # if this is false, there is a big issue\n",
    "    #print(config.vocab_size)\n",
    "    #print(conf_dict_data[\"vocab_size\"])\n",
    "    #assert conf_dict_data[\"vocab_size\"] == config.vocab_size, \"Vocab size mismatch\"\n",
    "    \n",
    "    config.update(conf_dict_data)\n",
    "    model.config = config\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4M = patch_model_from_tokenizer(model_4M, tokenizer_4M)\n",
    "model_19M = patch_model_from_tokenizer(model_19M, tokenizer_19M)\n",
    "#model_1B = patch_model_from_tokenizer(model_1B, tokenizer_1B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedModel\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import Pretr\n",
    "isinstance(tokenizer_4M, PreTrainedTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='ncfrey/ChemGPT-4.7M', vocab_size=684, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_4M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39032c33ef244ca4abcd5817e9c8cc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 07:33:42.673 | INFO     | molfeat.trans.pretrained.hf_transformers:save:49 - Model saved to gs://molfeat-store-dev/artifacts/huggingface/ChemGPT-4.7M/0/model.save\n",
      "2023-02-03 07:33:44.490 | INFO     | molfeat.store.modelstore:register:124 - Successfuly registered model ChemGPT-4.7M !\n"
     ]
    }
   ],
   "source": [
    "chempgtp_4M_model = HFModel.register_pretrained(model_4M, tokenizer_4M, chemgpt_4M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93d217e871e4b07a223927ee616404a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 07:35:53.545 | INFO     | molfeat.trans.pretrained.hf_transformers:save:49 - Model saved to gs://molfeat-store-dev/artifacts/huggingface/ChemGPT-1.2B/0/model.save\n",
      "2023-02-03 07:35:55.348 | INFO     | molfeat.store.modelstore:register:124 - Successfuly registered model ChemGPT-1.2B !\n"
     ]
    }
   ],
   "source": [
    "chempgtp_19M_model = HFModel.register_pretrained(model_19M, tokenizer_19M, chemgpt_19M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<molfeat.trans.pretrained.hf_transformers.HFModel at 0x17597f760>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chempgtp_19M_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "molfeat-core",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd64925fe6617865d410306d2b64fa69b44b63a36aad85fd11f7d4e4dc7609f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
